{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d2c85e5-1961-4d4a-b23c-1ccb7f1b0d11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install uv\n",
    "!uv sync --active --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "011cb65a-0081-4cd9-839f-f53e04d94dd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import toml\n",
    "import requests, zipfile, io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f36dd3f-aa7e-41ed-bdc2-f3dc8223ba7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "env_vars = toml.load(\"../../conf/env_vars.toml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "160e44eb-1175-4f36-94dd-20ed34aca7a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Request test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32ae6791-2d60-4a35-bd08-a16083bcfa17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data SUS API link\n",
    "data_sus_api_link = \"https://opendatasus.saude.gov.br/api/3/action\"\n",
    "\n",
    "# Make the HTTP request\n",
    "response = requests.get(f\"{data_sus_api_link}/package_list\")\n",
    "\n",
    "# Use the json module to load CKAN's response into a dictionary\n",
    "response_dict = json.loads(response.content)\n",
    "\n",
    "# Check the contents of the response\n",
    "assert response_dict['success'] is True  # make sure if response is OK\n",
    "\n",
    "datasets = response_dict['result']         # extract all the packages from the response\n",
    "print(\"Total datasets: \", len(datasets))                       # print the total number of datasets\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "772cc362-9f09-474a-b2d9-f446613125a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_most_recent_data_url(package_id:str):\n",
    "\n",
    "    # Base url for package information. \n",
    "    data_sus_api_link = 'https://opendatasus.saude.gov.br/api/3/action'\n",
    "\n",
    "    # Construct the url for the package id.\n",
    "    package_information_url = f\"{data_sus_api_link}/package_show?id={package_id}\"\n",
    "\n",
    "    # Make the HTTP request\n",
    "    package_information = requests.get(package_information_url)\n",
    "\n",
    "    # Use the json module to load CKAN's response into a dictionary\n",
    "    package_dict = json.loads(package_information.content)\n",
    "\n",
    "    # Check the contents of the response.\n",
    "    assert package_dict['success'] is True  # Make sure if response is OK\n",
    "\n",
    "    for i in range(len(package_dict[\"result\"][\"resources\"])):\n",
    "        if (\n",
    "            package_dict[\"result\"][\"resources\"][i][\"format\"].lower() == \"csv\") & (\n",
    "            \"2025\" in package_dict[\"result\"][\"resources\"][i][\"name\"]\n",
    "            ):\n",
    "            url = package_dict[\"result\"][\"resources\"][i][\"url\"]\n",
    "            last_update = package_dict[\"result\"][\"resources\"][i][\"last_modified\"]\n",
    "        \n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a46fe2b3-0d5d-4c99-aaa0-0b09a2643ea7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2025 SRAG - Banco Vivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34c82989-b0ba-46d5-bea8-b58f870a3623",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SRAG package id.\n",
    "srag_package_id = \"39a4995f-4a6e-440f-8c8f-b00c81fae0d0\"\n",
    "latest_srag_update_url = get_most_recent_data_url(srag_package_id)\n",
    "latest_srag_update_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f443167-0775-499e-9dbd-6e3693955e59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " # Correct the file path to use the s3a protocol when reading the file using spark.\n",
    "corrected_srag_url = \"s3a:/\" + latest_srag_update_url.split(\"amazonaws.com\")[1]\n",
    "corrected_srag_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4adecf31-fe7f-4459-aaae-ffad9dd3d5de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "srag_table_name = F'{env_vars[\"CATALOG\"]}.{env_vars[\"SCHEMA\"]}.srag_vigilance'\n",
    "srag_table_2025 = F'{env_vars[\"CATALOG\"]}.{env_vars[\"SCHEMA\"]}.srag_2025'\n",
    "srag_schema = spark.read.table(srag_table_name).schema\n",
    "\n",
    "df_influenza_2025 = spark.read.options(delimiter=\";\", header=True).schema(srag_schema).csv(corrected_srag_url, dateFormat=\"dd/MM/yyyy\")\n",
    "\n",
    "# Save the new data to the SRAG table.\n",
    "df_influenza_2025.write.mode(\"overwrite\").saveAsTable(srag_table_2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5406923f-8c73-4d14-bce3-9eab1d8821bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_influenza_2025.toPandas().tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2710e0d2-8813-47bc-a2db-2d78dcb54441",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"df_influenza_2025: num_rows = \", df_influenza_2025.count(), \", num_cols = \", len(df_influenza_2025.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af4c7673-112b-4e5f-81c3-b3433deebe39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "srag_table = spark.read.table(srag_table_name)\n",
    "row_count_before, col_count_before = srag_table.count(), len(srag_table.columns)\n",
    "print(srag_table_name, \": num_rows = \", row_count_before, \", num_cols = \", col_count_before)\n",
    "\n",
    "# Append 2025 new data to srag table.\n",
    "spark.sql(f\"\"\"MERGE INTO {srag_table_name}\n",
    "USING {srag_table_2025}\n",
    "ON {srag_table_name}.NU_NOTIFIC = {srag_table_2025}.NU_NOTIFIC\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET *\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT *\n",
    "\"\"\")\n",
    "\n",
    "srag_table_after = spark.read.table(srag_table_name)\n",
    "print(srag_table_name, \"after merging new data: num_rows = \", srag_table_after.count(), \", num_cols = \", len(srag_table_after.columns))\n",
    "print(\"Number of new rows appended = \", srag_table_after.count() - srag_table.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19c75278-1ab4-49b3-9436-44a9045eb8a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2025 Hospital UTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64ea3e3a-81dc-4af2-831b-8112da68607d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# HOSPITAL package id.\n",
    "hospital_package_id = \"791730b2-50bd-41ba-adf2-d915b88f712a\"\n",
    "latest_hospital_update_url = get_most_recent_data_url(hospital_package_id)\n",
    "latest_hospital_update_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e9c5999-b358-4464-9073-b0fa3fc9101d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hospital_table_name = F'{env_vars[\"CATALOG\"]}.{env_vars[\"SCHEMA\"]}.hospital'\n",
    "hospital_table_name_2025 = F'{env_vars[\"CATALOG\"]}.{env_vars[\"SCHEMA\"]}.hospital_2025'\n",
    "hospital_schema = spark.read.table(hospital_table_name).schema\n",
    "hospital_columns = spark.read.table(hospital_table_name).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9faa0b15-83a8-4acb-8bc9-64e859e89eb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Download zip file into memory.\n",
    "r = requests.get(latest_hospital_update_url)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "\n",
    "# Pick the first CSV name of zip file.\n",
    "csv_filename = z.namelist()[0]\n",
    "\n",
    "# Load into Pandas directly from the zip with the correct encoding\n",
    "with z.open(csv_filename) as hospital_file:\n",
    "    pd_df_hospital = pd.read_csv(\n",
    "        hospital_file, \n",
    "        delimiter=\";\", \n",
    "        encoding='latin1', \n",
    "        on_bad_lines='skip'\n",
    "    )[hospital_columns]\n",
    "\n",
    "# Save the new hospital data into a. spark dataframe.\n",
    "df_hospital_2025 = spark.createDataFrame(pd_df_hospital, schema=hospital_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7331d053-834a-4b30-88e6-94558a299fb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd_df_hospital.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bc5ac74-d17b-4fe6-95c8-fb47d7abb602",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check column's match before merging the data.\n",
    "\n",
    "cols_df_hospital_uti_all = set(hospital_columns)\n",
    "cols_df_hospital_uti_2025 = set(df_hospital_2025.columns)\n",
    "\n",
    "print(\"Only in df_hospital_uti_all:\", cols_df_hospital_uti_2025 - cols_df_hospital_uti_all)\n",
    "print(\"Only in df_hospital_uti_all:\", cols_df_hospital_uti_all - cols_df_hospital_uti_2025)\n",
    "print(\"In all:\", len(cols_df_hospital_uti_all & cols_df_hospital_uti_2025))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd27bbbc-aaa2-4bf5-a4d8-70d33a235f10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the new data into a table.\n",
    "df_hospital_2025.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(hospital_table_name_2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b35d056b-7ead-4c64-8c5b-54ea6642875b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hospital_table_2025 = spark.read.table(hospital_table_name_2025)\n",
    "print(\"num_rows = \", hospital_table_2025.count())\n",
    "print(\"num_cols = \", len(hospital_table_2025.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7618d21c-5380-430f-aea1-0f3c338cdf0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hospital_table = spark.read.table(hospital_table_name)\n",
    "row_count_before, col_count_before = hospital_table.count(), len(hospital_table.columns)\n",
    "print(hospital_table_name, \": num_rows = \", row_count_before, \", num_cols = \", col_count_before)\n",
    "\n",
    "# Append 2025 new data to hospital table.\n",
    "spark.sql(f\"\"\"\n",
    "MERGE INTO {hospital_table_name} AS target\n",
    "USING {hospital_table_name_2025} AS source\n",
    "ON target.CNES = source.CNES\n",
    "AND target.COMP = source.COMP\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET *\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT *\n",
    "\"\"\")\n",
    "\n",
    "hospital_table_after = spark.read.table(hospital_table_name)\n",
    "print(hospital_table_name, \"after merging new data: num_rows = \", hospital_table_after.count(), \", num_cols = \", len(hospital_table_after.columns))\n",
    "print(\"Number of new rows appended = \", hospital_table_after.count() - row_count_before)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "extract_refreshing_data.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
