{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8be79ac4-5ee7-45bb-b2b8-feced9028e04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install uv\n",
    "!uv add databricks-langchain\n",
    "!uv sync --active --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6709c52-cd51-4283-851d-2a20901f575f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import toml\n",
    "from typing import Optional\n",
    "from pyspark.sql.connect.dataframe import DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1699089-7e0b-4fc7-b13a-497f72e1360e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load environment variables.\n",
    "env_vars = toml.load(\"../../conf/env_vars.toml\")\n",
    "\n",
    "# Set as environment variables.\n",
    "for key, value in env_vars.items():\n",
    "    os.environ[key] = str(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "398ecfa4-7f60-4d9e-a928-f4fc151efd41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "srag_df = spark.read.table(F'{env_vars[\"CATALOG\"]}.{env_vars[\"FS_SCHEMA\"]}.srag_features')\n",
    "hospital_df = spark.read.table(F'{env_vars[\"CATALOG\"]}.{env_vars[\"FS_SCHEMA\"]}.hospital_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d6fe4e8-162b-4ccd-b9ea-d29dd5dc0a99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SRAGMetrics:\n",
    "    def __init__(\n",
    "        self, \n",
    "        df_srag: Optional[DataFrame] = None, \n",
    "        df_hospital: Optional[DataFrame] = None):\n",
    "        \"\"\"\n",
    "        df_srag: optional main dataset (e.g., monthly cases)\n",
    "        df_hospital: optional secondary dataset (e.g., hospitalizations)\n",
    "        \"\"\"\n",
    "        catalog = os.environ[\"CATALOG\"]\n",
    "        schema = os.environ[\"FS_SCHEMA\"]\n",
    "        self.df_srag = df_srag if df_srag is not None else spark.read.table(f'{catalog}.{schema}.srag_features')\n",
    "\n",
    "        self.df_hospital = df_hospital if df_hospital is not None else spark.read.table(f'{catalog}.{schema}.hospital_features')\n",
    "\n",
    "    # Metric functions\n",
    "    def calculate_cases_per_month(\n",
    "        self, \n",
    "        start_date: Optional[str] = None, \n",
    "        end_date: Optional[str] = None\n",
    "        ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculates number of cases per month. If start and end_dates are not provided, use last 12 months.\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): Spark DataFrame with column DT_NOTIFIC (date).\n",
    "            start_date (str): Start date in 'yyyy-MM-dd'. If None, defaults to 12 months ago.\n",
    "            end_date (str): End date in 'yyyy-MM-dd'. If None, defaults to today.\n",
    "            \n",
    "        Returns:\n",
    "            Pandas DataFrame with ['year_month', 'count'].\n",
    "        \"\"\"\n",
    "        # Default period = last 12 months.\n",
    "        if end_date is None:\n",
    "            end_date = pd.to_datetime(\"today\").strftime(\"%Y-%m-%d\")\n",
    "        if start_date is None:\n",
    "            start_date = (pd.to_datetime(end_date) - pd.DateOffset(months=12)).strftime(\"%Y-%m-%d\")\n",
    "        # Filter Spark DataFrame.\n",
    "        df_filtered = self.df_srag.filter((F.col(\"DT_NOTIFIC\") >= F.lit(start_date)) & \n",
    "                            (F.col(\"DT_NOTIFIC\") <= F.lit(end_date)))      \n",
    "        # Aggregate cases per month.\n",
    "        cases_per_month = (\n",
    "            df_filtered\n",
    "            .withColumn(\"year_month\", F.date_format(\"DT_NOTIFIC\", \"yyyy-MM\"))\n",
    "            .groupBy(\"year_month\")\n",
    "            .count()\n",
    "            .orderBy(\"year_month\")\n",
    "        )\n",
    "        # Convert to Pandas\n",
    "        cases_pd = cases_per_month.toPandas()\n",
    "        cases_pd[\"year_month\"] = pd.to_datetime(cases_pd[\"year_month\"])\n",
    "        return cases_pd\n",
    "    \n",
    "    def calculate_cases_per_month_variation_rate(\n",
    "        self,\n",
    "        cases_current_count: Optional[pd.DataFrame] = None, \n",
    "        cases_comparison_count: Optional[pd.DataFrame] = None\n",
    "        ) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the increase rate of cases per month. If cases_current_count and cases_comparison_count are not provided, use last month and 12 months ago.\n",
    "        \n",
    "        Args:\n",
    "            cases_current_count: number of srag cases in the current month. If None, defaults number of the last month.\n",
    "            cases_comparison_count: number of srag cases in the previous period to compare with. If None, defaults to 12 months ago.\n",
    "        Returns:\n",
    "            increase_rate (float): increase rate of cases compared to the previos period.\n",
    "        \"\"\"\n",
    "        last_month = pd.to_datetime(\"today\") - pd.DateOffset(months=1)\n",
    "        one_year_before = pd.to_datetime(\"today\") - pd.DateOffset(months=13)\n",
    "\n",
    "        if cases_current_count is None:\n",
    "            cases_current_count = self.calculate_cases_per_month(\n",
    "                start_date=pd.offsets.MonthBegin().rollback(last_month), \n",
    "                end_date=pd.offsets.MonthEnd().rollforward(last_month))[\"count\"][0]\n",
    "        if cases_comparison_count is None:\n",
    "            cases_comparison_count = self.calculate_cases_per_month(\n",
    "                start_date=pd.offsets.MonthBegin().rollback(one_year_before), \n",
    "                end_date=pd.offsets.MonthEnd().rollforward(one_year_before))[\"count\"][0]\n",
    "\n",
    "        variation_rate = ((cases_current_count - cases_comparison_count) / cases_comparison_count).round(2)*100\n",
    "        return variation_rate\n",
    "    \n",
    "    def calculate_cases_per_day(\n",
    "        self, \n",
    "        start_date: Optional[str] = None, \n",
    "        end_date: Optional[str] = None,\n",
    "        ) -> pd. DataFrame :\n",
    "        \"\"\"\n",
    "        Calculates number of cases per day.\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): Spark DataFrame with column DT_NOTIFIC (date) of the period of time to calculate the daily number of SRAG cases.\n",
    "            start_date (str): Start date in 'yyyy-MM-dd'. If None, defaults to 30 days interval.\n",
    "            end_date (str): End date in 'yyyy-MM-dd'. If None, defaults to today.\n",
    "            \n",
    "        Returns:\n",
    "            Pandas DataFrame with ['DT_NOTIFIC', 'count'].\n",
    "        \"\"\"\n",
    "        # Default period = last 30 days\n",
    "        if end_date is None:\n",
    "            end_date = pd.to_datetime(\"today\").strftime(\"%Y-%m-%d\")\n",
    "        if start_date is None:\n",
    "            start_date = (pd.to_datetime(end_date) - pd.DateOffset(days=30)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # Filter Spark DataFrame to the period of time desired.\n",
    "        filtered = self.df_srag.filter((F.col(\"DT_NOTIFIC\") >= F.lit(start_date)) & \n",
    "                            (F.col(\"DT_NOTIFIC\") <= F.lit(end_date)))\n",
    "        \n",
    "        # Aggregate cases per day\n",
    "        cases_per_day = (\n",
    "            filtered\n",
    "            .groupBy(\"DT_NOTIFIC\")\n",
    "            .count()\n",
    "            .orderBy(\"DT_NOTIFIC\")\n",
    "        )\n",
    "        \n",
    "        # Convert to Pandas\n",
    "        cases_per_day_pd = cases_per_day.toPandas()\n",
    "        cases_per_day_pd[\"DT_NOTIFIC\"] = pd.to_datetime(cases_per_day_pd[\"DT_NOTIFIC\"])\n",
    "\n",
    "        return cases_per_day_pd\n",
    "\n",
    "    # Agent-facing run method\n",
    "    def run(self, query: str = None) -> str:\n",
    "        \"\"\"Run function to calculate the metrics and return the results as a JSON string.\"\"\"\n",
    "        results = {\n",
    "            # \"cases_per_month\": self.calculate_cases_per_month().to_dict(orient=\"records\"),\n",
    "            \"calculate_cases_per_month_variation_rate\": self.calculate_cases_per_month_variation_rate(),\n",
    "            # \"hospitalizations\": self.get_hospitalizations().to_dict(orient=\"records\"),\n",
    "            # \"summary\": self.get_summary().to_dict(orient=\"records\")\n",
    "        }\n",
    "        return json.dumps(results, indent=2)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "metric_calculator.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
