{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7030c08b-0b52-4ca5-aafe-b70fc80876f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "https://python.langchain.com/v0.2/docs/integrations/tools/spark_sql/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cc71b60-a18d-4b1e-a953-f95f6d3bf92e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# !pip install uv\n",
    "# !uv add databricks-sql-connector databricks-sqlalchemy --active --quiet\n",
    "# !uv sync --active --quiet\n",
    "# dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7be261c4-f37b-423a-a9ff-bb8f98110a3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import sys\n",
    "import toml\n",
    "from dotenv import load_dotenv\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.agent_toolkits import SparkSQLToolkit, create_spark_sql_agent\n",
    "from langchain_community.utilities.spark_sql import SparkSQL\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools import BaseTool\n",
    "from pydantic import BaseModel\n",
    "from pyspark.sql import SparkSession\n",
    "from typing import Optional, Type, Any, Dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec90d30d-ac01-47d9-9a38-131e1c0f7a0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load environment variables.\n",
    "env_vars = toml.load(\"../../conf/env_vars.toml\")\n",
    "\n",
    "# Set as environment variables.\n",
    "for key, value in env_vars.items():\n",
    "    os.environ[key] = str(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05eba986-f830-4228-97a8-bab538b6e442",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load credentials variables from .env file.\n",
    "load_dotenv(\"../../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4726aea3-d44e-421e-9ca8-fa87ee668528",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SparkSQLQuerySchema(BaseModel):\n",
    "    query: str\n",
    "    max_results: Optional[int] = 100\n",
    "\n",
    "\n",
    "class SparkSQLQueryTool(BaseTool):\n",
    "    name: str = \"spark_sql_query_tool\"\n",
    "    description: str = (\n",
    "        \"\"\"A specialized agent designed to search and query structured databases to retrieve relevant information and insights from a SRAG dataset, using SparkSQLToolkit. It interprets natural language or structured prompts, translates them into optimized SQL or Spark SQL queries, executes them safely, and returns concise answers.\n",
    "        This agent is ideal for tasks involving:\n",
    "        - Data exploration and lookup across datasets.\n",
    "        - Data analysis and statistical calculations.\n",
    "        - Retrieving specific records or entities.\n",
    "        The agent returns results and saves verbose logs in memory.\"\"\"\n",
    "    )\n",
    "    memory: Optional[ConversationBufferMemory] = None\n",
    "    llm: Optional[ChatOpenAI] = None\n",
    "    toolkit: Optional[SparkSQLToolkit] = None\n",
    "    agent_executor: Any = None\n",
    "    args_schema: Type[BaseModel] = SparkSQLQuerySchema\n",
    "\n",
    "    def __init__(self, memory: Optional[ConversationBufferMemory] = None, **kwargs):\n",
    "        super().__init__(memory=memory, **kwargs)\n",
    "\n",
    "        # Initialize Spark\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        catalog = os.environ.get(\"CATALOG\")\n",
    "        schema = os.environ.get(\"FS_SCHEMA\")\n",
    "        spark.sql(f\"USE CATALOG {catalog}\")\n",
    "        spark.sql(f\"USE SCHEMA {schema}\")\n",
    "\n",
    "        # Memory for verbose logs\n",
    "        self.memory = memory or ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "\n",
    "        # LLM\n",
    "        OPENAI_API_KEY = os.getenv(\"OPEN_AI_KEY\")\n",
    "        llm = ChatOpenAI(api_key=OPENAI_API_KEY, temperature=0)\n",
    "\n",
    "        # Agent setup\n",
    "        spark_sql = SparkSQL(catalog=catalog, schema=schema)\n",
    "        self.toolkit = SparkSQLToolkit(db=spark_sql, llm=llm)\n",
    "        self.agent_executor = create_spark_sql_agent(\n",
    "            llm=llm,\n",
    "            toolkit=self.toolkit,\n",
    "            memory=self.memory,\n",
    "            verbose=True,\n",
    "            agent_executor_kwargs={\"memory\": memory, 'handle_parsing_errors': True},\n",
    "        )\n",
    "\n",
    "    def _run(self, query: str, max_results: int = 100) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run the SQL query using the SparkSQL agent executor.\n",
    "        Returns a dict with:\n",
    "            query: The query used as input to the agent executor.\n",
    "            results: The final answer provided by the agent executor\n",
    "            verbose: verbose logs from memory.\n",
    "        \"\"\"\n",
    "        buffer = io.StringIO()\n",
    "        stdout = sys.stdout\n",
    "        sys.stdout = buffer  # Redirect print output\n",
    "\n",
    "        try:\n",
    "            result = self.agent_executor.run(query)\n",
    "        except Exception as e:\n",
    "            result = []\n",
    "            self.memory.save_context({\"input\": query}, {\"output\": str(e)})\n",
    "        finally:\n",
    "            sys.stdout = stdout  # Restore stdout\n",
    "\n",
    "        # Store captured verbose text in memory for other agents\n",
    "        verbose_text = buffer.getvalue()\n",
    "        self.memory.save_context(\n",
    "            {\"query\": query},\n",
    "            {\"verbose_log\": verbose_text[:2000]}  # Truncate if large\n",
    "        )\n",
    "\n",
    "        # Limit result size\n",
    "        if isinstance(result, list):\n",
    "            result = result[:max_results]\n",
    "\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"results\": result,\n",
    "            \"verbose\": verbose_text\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "database_searcher.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
